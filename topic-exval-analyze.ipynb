{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# トピック分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "from book import *\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models.ldamodel import CoherenceModel\n",
    "from labMTsimple.storyLab import *\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = 'data/'\n",
    "DIST = 'dist/'\n",
    "TMP = 'tmp/'\n",
    "TEXT = 'text/'\n",
    "PICKLE = 'pickle/'\n",
    "MODEL = 'model/'\n",
    "\n",
    "SRC = 'sources.txt'\n",
    "TEST = 'sources-test.txt'\n",
    "MIN_TEXT = 'texts-min.txt'\n",
    "\n",
    "NUM_TOPICS = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labMT, labMTvector, labMTwordlist = emotionFileReader(returnVector=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_array(file, delim=None, cnt=None):\n",
    "    with open(file, 'r') as f:\n",
    "        if cnt:\n",
    "            return [[elem.strip() for elem in line.split(delim, cnt)]\n",
    "                    for line in f.readlines()]\n",
    "        else:\n",
    "            return [[elem.strip() for elem in line.split(delim)]\n",
    "                    for line in f.readlines()]\n",
    "\n",
    "\n",
    "def fprint_array(file_name, array):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for vector in array:\n",
    "            for elem in vector:\n",
    "                f.write('{} '.format(elem))\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['12', '12-cleaned.txt', 'Through the Looking-Glass'],\n",
      " ['16', '16-cleaned.txt', 'Peter Pan'],\n",
      " ['20', '20-cleaned.txt', 'Paradise Lost'],\n",
      " ['21', '21-cleaned.txt', 'Aesop’s Fables'],\n",
      " ['24', '24-cleaned.txt', 'O Pioneers!'],\n",
      " ['32', '32-cleaned.txt', 'Herland'],\n",
      " ['35', '35-cleaned.txt', 'The Time Machine'],\n",
      " ['36', '36-cleaned.txt', 'The War of the Worlds'],\n",
      " ['41', '41-cleaned.txt', 'The Legend of Sleepy Hollow'],\n",
      " ['45', '45-cleaned.txt', 'Anne of Green Gables']]\n"
     ]
    }
   ],
   "source": [
    "infos = load_array(DATA + SRC, ',', 2)\n",
    "pprint(infos[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_happiness(window):\n",
    "    return emotion(window, labMT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1254/1254 [08:05<00:00,  2.58it/s]\n"
     ]
    }
   ],
   "source": [
    "books = {}\n",
    "for book_id, file_name, title in tqdm(infos):\n",
    "    FILE = TMP + PICKLE + '{}.pickle'.format(book_id)\n",
    "    book = Book(book_id, title, file_name)\n",
    "    if not os.path.exists(FILE):\n",
    "        try:\n",
    "            book.load()\n",
    "            book.windowed()\n",
    "            book.calc_happiness(calc_happiness)\n",
    "            books[int(book_id)] = book\n",
    "            with open(FILE, 'wb') as f:\n",
    "                pickle.dump(book, f)\n",
    "        except BookLoadingException as err:\n",
    "            pass\n",
    "    else:\n",
    "        with open(FILE, 'rb') as f:\n",
    "            books[int(book_id)] = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n"
     ]
    }
   ],
   "source": [
    "print(len(books))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_window(book):\n",
    "    happinesses = book.happinesses()\n",
    "    i = happinesses.index(min(happinesses))\n",
    "    local_array = []\n",
    "    local_array.append(i)\n",
    "    for word in book.windows()[i].split():\n",
    "        local_array.append(word)\n",
    "    return local_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TEXT_PATH = TMP + MIN_TEXT\n",
    "if not os.path.exists(MIN_TEXT_PATH):\n",
    "    min_windows = [get_min_window(books[book_id]) for book_id in books]\n",
    "    fprint_array(MIN_TEXT_PATH, min_windows)\n",
    "else:\n",
    "    min_windows = load_array(MIN_TEXT_PATH)\n",
    "min_windows = [array[1:] for array in min_windows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1159\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(min_windows))\n",
    "print(len(min_windows[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "DICTIONARY_PATH = TMP + 'dictionary.dict'\n",
    "if os.path.exists(DICTIONARY_PATH):\n",
    "    dictionary = Dictionary.load(DICTIONARY_PATH)\n",
    "else:\n",
    "    dictionary = Dictionary(min_windows)\n",
    "    dictionary.filter_extremes(no_below=2, no_above=0.75)\n",
    "    dictionary.save(DICTIONARY_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [03:46<00:00, 15.08s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_FOLDER = TMP + MODEL\n",
    "if not os.path.exists(MODEL_FOLDER):\n",
    "    os.makedirs(MODEL_FOLDER)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in min_windows]\n",
    "models = {}\n",
    "\n",
    "for topic_num in tqdm(range(1, NUM_TOPICS)):\n",
    "    MODEL_PATH = MODEL_FOLDER + 'topic-{}.model'.format(topic_num)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        lda_model = LdaModel(corpus=corpus, num_topics=topic_num, id2word=dictionary)\n",
    "        lda_model.save(MODEL_PATH)\n",
    "    else:\n",
    "        lda_model = LdaModel.load(MODEL_PATH)\n",
    "    models[topic_num] = lda_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## テストデータで分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['37332', '37332-cleaned.txt', 'A Little Princess'],\n",
      " ['43', '43-cleaned.txt', 'The Strange Case Of Dr. Jekyll And Mr. Hyde'],\n",
      " ['11', '11-cleaned.txt', 'Alice’s Adventures in Wonderland'],\n",
      " ['98', '98-cleaned.txt', 'A Tale of Two Cities'],\n",
      " ['1974', '1974-cleaned.txt', 'Poetics']]\n"
     ]
    }
   ],
   "source": [
    "TEST_PATH = DATA + TEST\n",
    "test_infos = load_array(TEST_PATH, ',', 2)\n",
    "pprint(test_infos[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "test_books = {}\n",
    "for book_id, file_name, title in test_infos:\n",
    "    FILE = TMP + PICKLE + '{}.pickle'.format(book_id)\n",
    "    if not os.path.exists(FILE):\n",
    "        try:\n",
    "            book = Book(book_id, title, file_name)\n",
    "            book.load()\n",
    "            book.windowed()\n",
    "            book.calc_happiness(calc_happiness)\n",
    "            test_books[int(book_id)] = book\n",
    "            with open(FILE, 'wb') as f:\n",
    "                pickle.dump(book, f)\n",
    "        except BookLoadingException as err:\n",
    "            pass\n",
    "            \n",
    "    else:\n",
    "        with open(FILE, 'rb') as f:\n",
    "            test_books[int(book_id)] = pickle.load(f)\n",
    "print(len(test_books))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_windows = []\n",
    "labels = []\n",
    "for book_id in test_books:\n",
    "    book = test_books[book_id]\n",
    "    window = get_min_window(book)[1:]\n",
    "    test_windows.append(window)\n",
    "    labels.append(book.title())\n",
    "test_dictionary = Dictionary(test_windows)\n",
    "test_dictionary.filter_extremes(no_below=2, no_above=0.75)\n",
    "test_corpus = [test_dictionary.doc2bow(text) for text in test_windows]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for max_topic in range(1, NUM_TOPICS):\n",
    "    model = models[max_topic]\n",
    "    score_by_topic = defaultdict(int)\n",
    "    topic_array = [[0.0 for j in range(max_topic)] for i in range(len(labels))]\n",
    "    for index, unseen_doc in enumerate(test_corpus):\n",
    "        for topic, score in model[unseen_doc]:\n",
    "            topic_array[index][int(topic)] = float(score)\n",
    "    df = pd.DataFrame(topic_array)\n",
    "    df.to_excel(DIST + '{}.xlsx'.format(max_topic))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 評価\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexityによる評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = []\n",
    "for max_topic in range(1, NUM_TOPICS):\n",
    "    model = models[max_topic]\n",
    "    perwordbound = model.log_perplexity(test_corpus)\n",
    "    perplexities.append([max_topic, np.exp2(-perwordbound)])\n",
    "fprint_array(DIST + 'perplexity.txt', perplexities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coherenceによる評価\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1567",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9314646ede9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mdictionary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                         coherence='c_v')\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mcoherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mcoherences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mfprint_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDIST\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'coherence.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoherences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \"\"\"\n\u001b[0;32m--> 609\u001b[0;31m         \u001b[0mconfirmed_measures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_coherence_per_topic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate_measures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfirmed_measures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mget_coherence_per_topic\u001b[0;34m(self, segmented_topics, with_std, with_support)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0msegmented_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msegmented_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_std\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwith_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/models/coherencemodel.py\u001b[0m in \u001b[0;36mestimate_probabilities\u001b[0;34m(self, segmented_topics)\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyed_vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeasure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/topic_coherence/probability_estimation.py\u001b[0m in \u001b[0;36mp_boolean_sliding_window\u001b[0;34m(texts, segmented_topics, dictionary, window_size, processes)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0maccumulator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"using %s to estimate probabilities from sliding windows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocesses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelWordOccurrenceAccumulator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             raise ValueError(\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[0;32m--> 288\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWindowedTextsAnalyzer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_none_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vocab_size\u001b[0m  \u001b[0;31m# see _iter_texts for use of none token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, relevant_ids, dictionary)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \"\"\"\n\u001b[1;32m    190\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUsesDictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ids_to_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelevant_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken2id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Volumes/macos/Users/tomoya/Documents/片寄研究室/python/topic_analyze/venv/lib/python3.7/site-packages/gensim/topic_coherence/text_analysis.py\u001b[0m in \u001b[0;36m_ids_to_words\u001b[0;34m(ids, dictionary)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1567"
     ]
    }
   ],
   "source": [
    "coherences = []\n",
    "for i in range(1, NUM_TOPICS):\n",
    "    lda = models[i]\n",
    "\n",
    "    cm = CoherenceModel(model=lda, \n",
    "                        texts=test_windows, \n",
    "                        dictionary=test_dictionary, \n",
    "                        coherence='c_v')\n",
    "    coherence = cm.get_coherence()\n",
    "    coherences.append([i, coherence])\n",
    "fprint_array(DIST + 'coherence.txt', coherences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "ax1.plot([a[1] for a in coherences], color='r')\n",
    "ax2.plot([perplexity[1] for perplexity in perplexities], color='b')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
