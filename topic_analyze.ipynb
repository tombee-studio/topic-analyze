{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 物語の感情曲線の抽出とその極値に着目したトピックの分析\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pyLDAvis.gensim\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import *\n",
    "from labMTsimple.storyLab import *\n",
    "from book import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from mvpa2.suite import SimpleSOMMapper\n",
    "from pyvis.network import Network\n",
    "from collections import defaultdict\n",
    "\n",
    "sns.set(style='darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST = 'dist/'\n",
    "DATA = 'data/'\n",
    "TMP = 'tmp/'\n",
    "PICKLE = 'pickle/'\n",
    "CLUSTERS = 'clusters/'\n",
    "TF_IDF = 'tf_idf/'\n",
    "TF = 'tf/'\n",
    "IDF = 'idf/'\n",
    "SRC = 'sources-mini.txt'\n",
    "TEST = 'sources-test.txt'\n",
    "WORDS_IN_WINDOW = 'words-window.txt'\n",
    "WORD_DICT = 'word-dict.dict'\n",
    "WORDS_CORPUS = 'words-corpus.mm'\n",
    "EXCLUDE_RATE = 0.1\n",
    "NUM_TOPICS = 500\n",
    "labMT, labMTvector, labMTwordList = emotionFileReader(returnVector=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15718', 'How To Write Special Feature Articles', '15718-cleaned.txt']\n",
      "['16751', \"McGuffey's Sixth Eclectic Reader\", '16751-cleaned.txt']\n",
      "['42474', '1000 Mythological Characters Briefly Described', '42474-cleaned.txt']\n",
      "['51079', 'Ned, Bob and Jerry at Boxwood Hall', '51079-cleaned.txt']\n",
      "['28617', 'Astounding Stories of Super-Science February 1930', '28617-cleaned.txt']\n"
     ]
    }
   ],
   "source": [
    "# sources.txtを読み込みます\n",
    "infos = []\n",
    "books = []\n",
    "with open(DATA + SRC, 'r') as i:\n",
    "    lines = i.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        book_id, path, title = line.split(',', 2)\n",
    "        infos.append([book_id, title, path])\n",
    "for info in infos[:5]:\n",
    "    print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_happiness(window):\n",
    "    return emotion(window, labMT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 1/100 [00:00<00:32,  3.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "  2%|▏         | 2/100 [00:00<00:31,  3.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "  3%|▎         | 3/100 [00:00<00:29,  3.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "  4%|▍         | 4/100 [00:01<00:26,  3.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "  5%|▌         | 5/100 [00:01<00:25,  3.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "  6%|▌         | 6/100 [00:01<00:26,  3.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "  7%|▋         | 7/100 [00:01<00:25,  3.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "  8%|▊         | 8/100 [00:02<00:25,  3.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "  9%|▉         | 9/100 [00:02<00:24,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "The Adventure of the Cardboard Boxは12000字を超えていません:   9%|▉         | 9/100 [00:02<00:24,  3.74it/s]\u001b[A\u001b[A\n",
      "\n",
      "The Adventure of the Cardboard Boxは12000字を超えていません:  11%|█         | 11/100 [00:02<00:21,  4.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "The Adventure of the Cardboard Boxは12000字を超えていません:  12%|█▏        | 12/100 [00:03<00:22,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "The Adventure of the Cardboard Boxは12000字を超えていません:  13%|█▎        | 13/100 [00:03<00:20,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Alice in Wonderlandは12000字を超えていません:  13%|█▎        | 13/100 [00:03<00:20,  4.15it/s]               \u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  13%|█▎        | 13/100 [00:03<00:20,  4.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  15%|█▌        | 15/100 [00:03<00:15,  5.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  16%|█▌        | 16/100 [00:03<00:19,  4.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  17%|█▋        | 17/100 [00:04<00:20,  4.09it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  18%|█▊        | 18/100 [00:04<00:21,  3.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  19%|█▉        | 19/100 [00:04<00:21,  3.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  20%|██        | 20/100 [00:05<00:34,  2.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  21%|██        | 21/100 [00:05<00:31,  2.51it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  22%|██▏       | 22/100 [00:06<00:28,  2.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  23%|██▎       | 23/100 [00:06<00:26,  2.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  24%|██▍       | 24/100 [00:06<00:25,  3.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  25%|██▌       | 25/100 [00:06<00:23,  3.15it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  26%|██▌       | 26/100 [00:07<00:21,  3.37it/s]\u001b[A\u001b[A\n",
      "\n",
      "Moral Principles in Educationは12000字を超えていません:  27%|██▋       | 27/100 [00:07<00:22,  3.29it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  27%|██▋       | 27/100 [00:07<00:22,  3.29it/s]                        \u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  29%|██▉       | 29/100 [00:07<00:18,  3.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  30%|███       | 30/100 [00:08<00:19,  3.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  31%|███       | 31/100 [00:08<00:19,  3.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  32%|███▏      | 32/100 [00:08<00:19,  3.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  33%|███▎      | 33/100 [00:09<00:20,  3.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  34%|███▍      | 34/100 [00:09<00:19,  3.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "Poemsは12000字を超えていません:  35%|███▌      | 35/100 [00:09<00:19,  3.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  35%|███▌      | 35/100 [00:09<00:19,  3.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  37%|███▋      | 37/100 [00:09<00:15,  4.20it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  38%|███▊      | 38/100 [00:10<00:15,  4.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  39%|███▉      | 39/100 [00:10<00:13,  4.43it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  40%|████      | 40/100 [00:10<00:14,  4.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  41%|████      | 41/100 [00:10<00:15,  3.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  42%|████▏     | 42/100 [00:11<00:15,  3.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  43%|████▎     | 43/100 [00:11<00:15,  3.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Orphans of the Voidは12000字を超えていません:  44%|████▍     | 44/100 [00:11<00:14,  3.79it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  44%|████▍     | 44/100 [00:11<00:14,  3.79it/s]          \u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  46%|████▌     | 46/100 [00:11<00:12,  4.39it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  47%|████▋     | 47/100 [00:12<00:13,  4.04it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  48%|████▊     | 48/100 [00:12<00:13,  3.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  49%|████▉     | 49/100 [00:12<00:13,  3.80it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  50%|█████     | 50/100 [00:13<00:12,  3.93it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  51%|█████     | 51/100 [00:13<00:12,  4.05it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  52%|█████▏    | 52/100 [00:13<00:13,  3.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  53%|█████▎    | 53/100 [00:13<00:12,  3.81it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  54%|█████▍    | 54/100 [00:14<00:11,  3.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  55%|█████▌    | 55/100 [00:14<00:11,  3.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  56%|█████▌    | 56/100 [00:14<00:12,  3.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  57%|█████▋    | 57/100 [00:14<00:12,  3.57it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  58%|█████▊    | 58/100 [00:15<00:11,  3.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  59%|█████▉    | 59/100 [00:15<00:11,  3.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  60%|██████    | 60/100 [00:15<00:11,  3.41it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  61%|██████    | 61/100 [00:16<00:11,  3.48it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  62%|██████▏   | 62/100 [00:16<00:10,  3.76it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  63%|██████▎   | 63/100 [00:16<00:12,  2.86it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  64%|██████▍   | 64/100 [00:17<00:11,  3.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  65%|██████▌   | 65/100 [00:17<00:11,  3.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  66%|██████▌   | 66/100 [00:17<00:09,  3.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  67%|██████▋   | 67/100 [00:17<00:07,  4.18it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  68%|██████▊   | 68/100 [00:18<00:08,  3.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  69%|██████▉   | 69/100 [00:18<00:08,  3.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  70%|███████   | 70/100 [00:18<00:08,  3.73it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  71%|███████   | 71/100 [00:18<00:07,  3.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  72%|███████▏  | 72/100 [00:19<00:07,  3.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  73%|███████▎  | 73/100 [00:19<00:07,  3.84it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  74%|███████▍  | 74/100 [00:19<00:07,  3.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  75%|███████▌  | 75/100 [00:19<00:06,  3.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  76%|███████▌  | 76/100 [00:20<00:06,  3.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  77%|███████▋  | 77/100 [00:20<00:05,  4.01it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  78%|███████▊  | 78/100 [00:20<00:05,  3.90it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  79%|███████▉  | 79/100 [00:20<00:05,  3.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  80%|████████  | 80/100 [00:21<00:05,  3.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  81%|████████  | 81/100 [00:21<00:05,  3.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  82%|████████▏ | 82/100 [00:21<00:04,  3.78it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  83%|████████▎ | 83/100 [00:22<00:04,  3.66it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  84%|████████▍ | 84/100 [00:22<00:04,  3.75it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  85%|████████▌ | 85/100 [00:22<00:04,  3.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  86%|████████▌ | 86/100 [00:22<00:03,  3.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  87%|████████▋ | 87/100 [00:23<00:03,  3.46it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  88%|████████▊ | 88/100 [00:23<00:03,  3.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  89%|████████▉ | 89/100 [00:23<00:02,  3.70it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  90%|█████████ | 90/100 [00:23<00:02,  3.85it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  91%|█████████ | 91/100 [00:24<00:02,  3.30it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  92%|█████████▏| 92/100 [00:24<00:02,  3.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  93%|█████████▎| 93/100 [00:24<00:02,  3.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  94%|█████████▍| 94/100 [00:25<00:01,  3.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "Contagionは12000字を超えていません:  95%|█████████▌| 95/100 [00:25<00:01,  3.45it/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "McGuffey's First Eclectic Reader, Revised Editionは12000字を超えていません:  95%|█████████▌| 95/100 [00:25<00:01,  3.45it/s]\u001b[A\u001b[A\n",
      "\n",
      "McGuffey's First Eclectic Reader, Revised Editionは12000字を超えていません:  97%|█████████▋| 97/100 [00:25<00:00,  4.03it/s]\u001b[A\u001b[A\n",
      "\n",
      "McGuffey's First Eclectic Reader, Revised Editionは12000字を超えていません:  98%|█████████▊| 98/100 [00:26<00:00,  3.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "McGuffey's First Eclectic Reader, Revised Editionは12000字を超えていません:  99%|█████████▉| 99/100 [00:26<00:00,  3.71it/s]\u001b[A\u001b[A\n",
      "\n",
      "McGuffey's First Eclectic Reader, Revised Editionは12000字を超えていません: 100%|██████████| 100/100 [00:26<00:00,  3.75it/s]\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "progress = tqdm(infos)\n",
    "for info in progress:\n",
    "    PICKLE_FILE = TMP + PICKLE + info[0] + '.pickle'\n",
    "    if os.path.exists(PICKLE_FILE):\n",
    "        with open(PICKLE_FILE, 'rb') as i:\n",
    "            books.append(pickle.load(i))\n",
    "    else:\n",
    "        book = Book(info[0], info[1], info[2])\n",
    "        try:\n",
    "            book.load()\n",
    "            book.windowed()\n",
    "            book.calc_happiness(calc_happiness)\n",
    "            with open(PICKLE_FILE, 'wb') as o:\n",
    "                pickle.dump(book, o)\n",
    "            books.append(book)\n",
    "        except BookLoadingException as err:\n",
    "            progress.set_description('{}は12000字を超えていません'.format(book.title()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_min_window(book):\n",
    "    happinesses = book.happinesses()\n",
    "    index = happinesses.index(min(happinesses))\n",
    "    return index, book.windows()[index]\n",
    "\n",
    "\n",
    "def get_max_window(book):\n",
    "    happinesses = book.happinesses()\n",
    "    index = happinesses.index(max(happinesses))\n",
    "    return index, book.windows()[index]\n",
    "\n",
    "\n",
    "def tf(book, index):\n",
    "    FOLDER = TMP + TF_IDF + TF + '{}/'.format(book.book_id())\n",
    "    if not os.path.exists(FOLDER):\n",
    "        os.makedirs(FOLDER)\n",
    "    FILE = '{}.txt'.format(index)\n",
    "    PATH = FOLDER + FILE\n",
    "    window = book.windows()[index].split()\n",
    "    if not os.path.exists(PATH):\n",
    "        word_freq = defaultdict(int)\n",
    "        word_set = set(window)\n",
    "        for word in word_set:\n",
    "            word_freq[word] = window.count(word)\n",
    "        with open(PATH, 'w') as o:\n",
    "            for word in word_freq:\n",
    "                o.write('{} {}\\n'.format(word, word_freq[word]))\n",
    "    else:\n",
    "        with open(PATH, 'r') as o:\n",
    "            word_freq = {}\n",
    "            for line in o.readlines():\n",
    "                word, freq = line.split()\n",
    "                word_freq[word] = int(freq)\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def idf(book, index):\n",
    "    FOLDER = TMP + TF_IDF + IDF + '{}/'.format(book.book_id())\n",
    "    if not os.path.exists(FOLDER):\n",
    "        os.makedirs(FOLDER)\n",
    "    FILE = '{}.txt'.format(index)\n",
    "    PATH = FOLDER + FILE\n",
    "    window = book.windows()[index].split()\n",
    "    if not os.path.exists(PATH):\n",
    "        word_freq = defaultdict(int)\n",
    "        word_set = set(window)\n",
    "        for word in word_set:\n",
    "            word_freq[word] = len(list(filter(lambda w: word in w.split(), book.windows())))\n",
    "        with open(PATH, 'w') as o:\n",
    "            for word in word_freq:\n",
    "                o.write('{} {}\\n'.format(word, word_freq[word]))\n",
    "    else:\n",
    "        with open(PATH, 'r') as o:\n",
    "            word_freq = {}\n",
    "            for line in o.readlines():\n",
    "                word, freq = line.split()\n",
    "                word_freq[word] = int(freq)\n",
    "    return word_freq\n",
    "\n",
    "\n",
    "def vectorize(book, index):\n",
    "    VECTOR_CSV = TMP + TF_IDF + '{}.csv'.format(book.book_id())\n",
    "    if os.path.exists(VECTOR_CSV):\n",
    "        vectors = pd.read_csv(VECTOR_CSV)\n",
    "    else:\n",
    "        corpus = book.windows()\n",
    "        vectorizer = TfidfVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        vectors = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "        vectors.to_csv(VECTOR_CSV)\n",
    "    return vectors[index:index+1]\n",
    "\n",
    "\n",
    "def tf_idf(book, index):\n",
    "    tfs = tf(book, index)\n",
    "    idfs = idf(book, index)\n",
    "    vector = vectorize(book, index)\n",
    "    return vector\n",
    "\n",
    "\n",
    "def remove_general_words(window, vector):\n",
    "    window = window.split()\n",
    "    general_words = {word: value for word, value in \n",
    "                     vector[:int(len(vector) * EXCLUDE_RATE)]}\n",
    "    too_reare_words = {word: value for word, value in \n",
    "                     vector[-int(len(vector) * EXCLUDE_RATE):]}\n",
    "    return [word for word in window \n",
    "            if word not in general_words.keys() \n",
    "            and word not in too_reare_words.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_experimental(book):\n",
    "    (index, window) = get_min_window(book)\n",
    "    vector = tf_idf(book, index)\n",
    "    vector_dict = {key: float(vector[key]) for key in vector.to_dict()\n",
    "                   if key != 'Unnamed: 0' and float(vector[key]) != 0.0}\n",
    "    vector_dict = sorted(vector_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "    return remove_general_words(window, vector_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/93 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "WORDS_IN_WINDOW_PATH = TMP + WORDS_IN_WINDOW\n",
    "if not os.path.exists(WORDS_IN_WINDOW_PATH):\n",
    "    texts = [get_experimental(book) for book in tqdm(books)]\n",
    "    with open(WORDS_IN_WINDOW_PATH, 'w') as i:\n",
    "        for words in texts:\n",
    "            for word in words:\n",
    "                i.write(word + ' ')\n",
    "            i.write('\\n')\n",
    "else:\n",
    "    with open(WORDS_IN_WINDOW_PATH, 'r') as i:\n",
    "        texts = [[word for word in line.split()] \n",
    "                 for line in tqdm(i.readlines())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(text) for text in texts]\n",
    "print(len(texts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Organizing MapおよびWard法による階層的クラスタリング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\n",
    "vectors = vectorizer.fit_transform(np.array(texts))\n",
    "pprint(type(vectors.toarray()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectors.toarray()\n",
    "print(X.shape)\n",
    "pca = PCA()\n",
    "X = pca.fit_transform(X)\n",
    "print(type(X))\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [book.title() for book in books]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 32\n",
    "n_cols = 32\n",
    "som = SimpleSOMMapper((n_rows, n_cols), 500, learning_rate=0.05)\n",
    "som.train(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for vector in som.K.T:\n",
    "    plt.imshow(vector)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "Z = linkage(vectors.toarray(), method='ward', metric='euclidean')\n",
    "threshold = 0.6*max(Z[:, 2])\n",
    "dendrogram(Z, labels=labels, orientation='left', color_threshold=threshold)\n",
    "plt.title('WARD METHOD')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(Z[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = fcluster(Z, threshold, criterion='distance')\n",
    "pprint(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "with open(TMP + CLUSTERS + 'clusters.txt', 'w') as f, \\\n",
    "        open(TMP + CLUSTERS + 'texts.txt', 'w') as o:\n",
    "    for book, text, cls in zip(books, texts, c):\n",
    "        if cls not in clusters:\n",
    "            clusters[cls] = []\n",
    "        clusters[cls].append((book, text))\n",
    "        f.write('{} {}\\n'.format(book.book_id(), cls))\n",
    "        o.write('{} {}\\n'.format(book.book_id(), text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クラスター内でのトピック分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = {}\n",
    "books_dict = {book.book_id(): book for book in books}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TMP + CLUSTERS + 'clusters.txt', 'r') as c, open(TMP + CLUSTERS + 'texts.txt', 'r') as t:\n",
    "    for line1, line2 in zip(c.readlines(), t.readlines()):\n",
    "        book_id, cls = line1.split()\n",
    "        book_id, text = line2.split(None, 1)\n",
    "        book_id = int(book_id)\n",
    "        cls = int(cls)\n",
    "        cluster = (books_dict[book_id], text.strip())\n",
    "        if cls not in clusters:\n",
    "            clusters[cls] = []\n",
    "        clusters[cls].append(cluster)\n",
    "for cls in clusters:\n",
    "    print('CLASS: {}'.format(cls))\n",
    "    for book, text in clusters[cls][:3]:\n",
    "        print('  {}'.format(book))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[t for book, t in clusters[i]] for i in clusters]\n",
    "for text in texts:\n",
    "    print(' '.join(text[0].split()[:3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共起ネットワークを構築する\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_network(df):\n",
    "    net = Network(height=\"1000px\", width=\"95%\", bgcolor=\"#FFFFFF\", font_color=\"black\", notebook=True)\n",
    "    print(df)\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame([[1.2, 2.3, 3.1, 0.0], [1.2, 2.3, 3.1, 1.4]], columns=['hello', 'i', 'happy', 'this'])\n",
    "net = generate_network(test_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
